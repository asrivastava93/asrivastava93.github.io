<!DOCTYPE HTML>
<html>
	<head>
		<title>Amber Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="on-preload">
		<!-- Wrapper -->
			<div id="container">
				
				<!-- Header -->
					<!-- <header id="header" class="alt">
						 -->
						 
					<img src="images/AmberSrivastava.jpeg" alt="Amber Srivastava" class="amber" />
						<div id="wrapper">
						<header id="header" class="alt">
						<h1>Amber Srivastava</h1>
						<h2> Postdoctoral Researcher </h2>
							<a href="https://ee.ethz.ch">Department of Information Technology and Electrical Engineering</a> <br />
							<a href="https://control.ee.ethz.ch">Automatic Control Laboratory</a> <br />
							<a href="https://ethz.ch/en.html">ETH Z&uuml;rich</a><br/>
							<a href="mailto:asrivastava@control.ee.ethz.ch">asrivastava@control.ee.ethz.ch</a>
</header>
</div>
</div>
					

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="index.html" >Home</a></li>
							<li><a href="News.html" class="active">News</a></li>

							<li><a href="research.html">Research</a></li>
							<li><a href="publications.html">Publications</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>News</h2>
										</header>
										<div>
											<ul>
												<li><p><b>05/2022:</b> Our paper "On the choice of Number of Superstates in the Aggregation of Markov Chains" has been accepted for publication in <a href="https://doi.org/10.1016/j.patrec.2022.05.019">Pattern Recognition Letters</a>. This paper proposes a notion of <b>Marginal Return</b> to compare different aggregated models of a Markov chain, and appropriately identify the best representative model.</p>
											  </li>
											
												<li><p><b>03/2022:</b> Our paper "Time-Varying Parameters in Sequential Decision Making Problems" has been posted on <a href="https://doi.org/10.48550/arXiv.2201.10273">arXiv</a>. This paper develops a control-theoretic framework to design the unknown parameter dynamics and the time-varying decision policy of the underlying sequential decision making task.</p>
											  </li>
											
												<li><p><b>08/2021:</b> Our paper "Parameterized MDPs and Reinforcement Learning Problems-A Maximum Entropy Principle-Based Framework" has been published on <a href="https://ieeexplore.ieee.org/document/9517030/">IEEE Transactions on Cybernetics</a>. This work proposes and builds a framework for a new <I>parameterized Sequential Decision Making </i> (<b> para-SDM</b>)class of problems with applications in diverse fields such as - network science, operations research, clustering and classification.</p>
											  </li>
											
											
												<li><p><b>08/2021:</b> I started as a postdoctoral researcher with <a href="http://people.ee.ethz.ch/~rsmith/">Dr. Roy Smith</a> at the Automatic Control Laboratory, ETH Z&uuml;rich. I will be working on modelling, estimation and control of large-scale networks. </p>
											  </li>
											
												<li><p><b>06/2021:</b> I defended my PhD thesis <b>Parameterized Sequential Decision Making</b> under the supervision of <a href="http://salapaka.web.engr.illinois.edu">Dr. Srinivasa M. Salapaka </a> at the <a href="https://csl.illinois.edu">Coordinated Science Laboratory, UIUC</a>. </p>
											  </li>
											
											
											<!--  <li><p><b>03/2022:</b> Our paper "Connectivity of the Feasible and Sublevel Sets of Dynamic Output Feedback Control with Robustness Constraints" has been posted <a href="https://arxiv.org/pdf/2203.11177.pdf">on arxiv</a>. This paper brings new insights for understanding policy optimization in the output feedback setting. </p>
											  </li> 
											  
											  
												<li><p><b>11/2021:</b> Our paper "Model-Free Î¼ Synthesis via Adversarial Reinforcement Learning" has been posted <a href="https://arxiv.org/pdf/2111.15537.pdf">on arxiv</a>. This paper builds a connection between adversarial reinforcement learning and the famous DK iteration algorithm from robust control.</p>
											   <p><b>Update:</b> The above paper has been accepted to American Control Conference (ACC) 2022.</p>
											  </li>
											  
											   <li><p><b>09/2021:</b> Our paper "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity" has been accepted to <a href="https://papers.nips.cc/paper/2021/hash/1714726c817af50457d810aae9d27a2e-Abstract.html">NeurIPS 2021</a>. </p>
											  </li>
											  
											   <li><p><b>03/2021:</b> Delighted to receive the <a href="https://www.amazon.science/research-awards/program-updates/2020-amazon-research-awards-recipients-announced">2020 Amazon Research Award</a>. Big thanks to Amazon!</p>
											  </li>
											  
											   <li><p><b>02/2021:</b> Thrilled to receive the <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=2048168&HistoricalAwards=false">NSF CAREER award</a> on "Interplay between Control Theory and Machine Learning." Big thanks to NSF!</p>
											  </li>
											  
											   <li><p><b>09/2020:</b> Our paper "On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems" has been accepted to <a href="https://proceedings.neurips.cc/paper/2020/hash/fb2e203234df6dee15934e448ee88971-Abstract.html">NeurIPS 2020</a>. </p>
											  </li>
											  
												 <li><p><b>03/2020:</b> Our paper "Analysis of Biased Stochastic Gradient Descent Using Sequential Semidefinite Programs" has been accepted to <a href="https://link.springer.com/article/10.1007%2Fs10107-020-01486-1">Mathematical Programming</a>. A full-text view-only version of the final paper can be found <a href="https://rdcu.be/b3b59">here</a>. </p>
											  </li>
											  
											   <li><p><b>10/2019:</b> Our paper "Policy Optimization for H2 Linear Control with H-infinity Robustness Guarantee: Implicit Regularization and Global Convergence" has been posted <a href="https://arxiv.org/pdf/1910.09496.pdf">on arxiv</a>. This paper studies the implicit regularization mechanism in policy-based reinforcement learning for robust control design.</p>
											  <p><b>Update I:</b> A conference version of the above paper has been accepted to <a href="https://sites.google.com/berkeley.edu/l4dc/home">L4DC 2020</a>. (one of 14/131 papers selected for oral presentation)</p>
												<p><b>Update II:</b> The journal version of the above paper has been accepted to SIAM Journal on Control and Optimization (SICON).</p>
											  </li>
											  
											  <li><p><b>06/2019:</b> Our paper "Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory" has been posted <a href="https://arxiv.org/pdf/1906.06781.pdf">on arxiv</a>. This is my first paper on analyzing reinforcement learning algorithms using control theory!</p>
											<p><b>Update:</b> The above paper has been accepted to <a href="https://nips.cc/Conferences/2019/Schedule?showEvent=13909">NeurIPS 2019</a>. The arxiv version of the paper has been revised.</p>
											  </li>
											  
											<li><p><b>08/2018:</b> I started as an Assistant Professor in the Electrical and Computer Engineering Department at the University of Illinois at Urbana-Champaign.</p>
											</li> -->
											</ul>
											
										
								</div>

							</section>

						
						

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>